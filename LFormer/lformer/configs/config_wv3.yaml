name: LFormer  # project_name
dataset_name: 'WV3'  # only support 'GF2', 'WV3', 'WV2', 'cave'
model_name: 'LFormer'
# train setting
alpha: 0.1
epoch_num: 500
batch_size: 32
base_lr: 3e-4
weight_decay: 1e-6
# parameters for scheduler lr
gamma: 0.1
step_size: 250
# model setting
pan_dim: 1
lms_dim: 8
attn_dim: 64
hp_dim: 64
n_stage: 5
crop_batch_size: 64
patch_size_list: [16, 64, 64]
scale: 4
patch_merge: True


data_path: '/home/LJL/Proj/pansharpening_data/training_wv3/train_wv3.h5'
log_dir: './logs'
# tensorboard file path
tb_log_path: './tb_logs'
weights_path: './weights/'
results_path: './results/'
gpu_list: [1]
workers: 0
save_epoch: 25

#test_mode: 'reduced'
#test_data_path: '/home/LJL/Proj/pansharpening_data/test data/h5/WV3/reduce_examples/test_wv3_multiExm1.h5'
test_mode: 'full'
test_data_path: '/home/LJL/Proj/pansharpening_data/test data/h5/WV3/full_examples/test_wv3_OrigScale_multiExm1.h5'
test_weight_path: './weights/WV3/20240903-152543/LFormer_epoch500.pth'
