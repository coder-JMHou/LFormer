name: LFormer  # project_name
dataset_name: 'GF2'  # only support 'GF2', 'WV3', 'WV2', 'cave'
model_name: 'LFormer'
# train setting
alpha: 0.1
epoch_num: 500
batch_size: 32
base_lr: 1e-3
weight_decay: 1e-6
# parameters for scheduler lr
gamma: 0.1
step_size: 250
# model setting
pan_dim: 1
lms_dim: 4   # 4 8 31
attn_dim: 64
hp_dim: 64
n_stage: 5
crop_batch_size: 64
patch_size_list: [16, 64, 64]
scale: 4
patch_merge: True


data_path: 'E:/HJM_Datasets/new_pan/training_gf2/train_gf2.h5'
log_dir: './logs'
# tensorboard file path
tb_log_path: './tb_logs'
weights_path: './weights/'
results_path: './results/'
gpu_list: [0]
workers: 0
save_epoch: 25

#test_mode: 'reduced'
#test_data_path: '/home/LJL/Proj/pansharpening_data/test data/h5/GF2/reduce_examples/test_gf2_multiExm1.h5'
test_mode: 'full'
test_data_path: '/home/LJL/Proj/pansharpening_data/test data/h5/GF2/full_examples/test_gf2_OrigScale_multiExm1.h5'
test_weight_path: './weights/lformer_R_gf2.pth'